{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.chdir('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "import nltk\n",
    "from functools import reduce\n",
    "!pip install wget\n",
    "# Load PyDrive and Google Auth related packages\n",
    "#!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from functools import reduce\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "import glove_helper\n",
    "\n",
    "# Load the json data\n",
    "def load_json_file(name):\n",
    "  \"\"\"\n",
    "  Load the json file and return a json object\n",
    "  \"\"\"\n",
    "  with open(name,encoding='utf-8') as myfile:\n",
    "    data = json.load(myfile)\n",
    "    return data\n",
    "\n",
    "# Convert json data object to a pandas data frame\n",
    "def convert_to_pd(data):\n",
    "  \"\"\"\n",
    "  Load the data to a pandas dataframe.\n",
    "  Dataframe Columns:\n",
    "    title\n",
    "    para_index\n",
    "    context\n",
    "    q_index\n",
    "    q_id\n",
    "    q_isimpossible\n",
    "    q_question\n",
    "    q_anscount - number of answers\n",
    "    q_answers - a list of object e.g [{ text: '', answer_start: 123}, ...]\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  for pdata in data['data']:\n",
    "    for para in pdata['paragraphs']:\n",
    "      for q in para['qas']:\n",
    "        result.append({\n",
    "            'title' : pdata['title'],\n",
    "            'context' : para['context'],\n",
    "            'q_id' : q['id'],\n",
    "            'q_isimpossible' : q['is_impossible'],\n",
    "            'q_question' : q['question'],\n",
    "            'q_anscount' : len(q['answers']),\n",
    "            'q_answers' : [a for a in q['answers']],\n",
    "            'q_answers_text': [a.get(\"text\") for a in q['answers']],\n",
    "            'context_lowercase': para['context'].lower(),\n",
    "            'q_question_lowercase' : q['question'].lower(),\n",
    "            'q_answers_text_lowercase': [a.get(\"text\").lower() for a in q['answers']],\n",
    "            \n",
    "        })\n",
    "\n",
    "  return pd.DataFrame.from_dict(result, orient='columns')\n",
    "\n",
    "# Load the file from shareable google drive link and return a pandas dataframe\n",
    "def loadDataFile(filename): \n",
    "  \"\"\"\n",
    "  Download a file from google drive with the shared link\n",
    "  \"\"\" \n",
    "  data = load_json_file(filename)\n",
    "  return convert_to_pd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONOT RUN THIS ON COLAB#\n",
    "#to make use of CPU and not GPU DONOT RUN THIS ON COLAB\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train-v2.0.json'\n",
    "dev_filename = 'dev-v2.0.json'\n",
    "\n",
    "train_pd = loadDataFile(train_filename)\n",
    "dev_pd = loadDataFile(dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max context length: 653\n",
      "Max question length: 40\n",
      "Max answer length: 43\n"
     ]
    }
   ],
   "source": [
    "def get_c_q_a(dataset):\n",
    "    q_id_list = []\n",
    "    context_list =[]\n",
    "    questions_list = []\n",
    "    answers_list =[]\n",
    "    q_impossible_list =[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        q_id_list.append(row.q_id)\n",
    "        context_list.append(row.context)\n",
    "        questions_list.append(row.q_question)\n",
    "        q_impossible_list.append(int(row.q_isimpossible))\n",
    "        if len(row.q_answers_text)>0 :\n",
    "            answers_list.append(row.q_answers_text[0])\n",
    "        else:\n",
    "            answers_list.append(\"\")\n",
    "    return [q_id_list,context_list,questions_list,q_impossible_list,answers_list]\n",
    "\n",
    "train_lists = get_c_q_a(train_pd)\n",
    "dev_lists = get_c_q_a(dev_pd)\n",
    "context_maxlen = max(map(len, (x.split() for x in train_lists[1])))\n",
    "question_maxlen = max(map(len, (x.split() for x in train_lists[2])))\n",
    "answer_maxlen = max(map(len, (x.split() for x in train_lists[4])))\n",
    "print(\"Max context length:\",context_maxlen)\n",
    "print(\"Max question length:\",question_maxlen)\n",
    "print(\"Max answer length:\",answer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_maxlen = 214\n",
    "question_maxlen = 18\n",
    "answer_maxlen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 88701\n",
      "validation num samples where answer impossible:  8730\n",
      "validation num samples where answer not impossible:  17382\n",
      "train num samples where answer impossible:  34761\n",
      "train num samples where answer not impossible:  69431\n"
     ]
    }
   ],
   "source": [
    "def tokenize_c_q_a(dataset,num_words=None):\n",
    "    tokenizer = Tokenizer(num_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"''\",oov_token='<unk>')\n",
    "    data = dataset[1]+dataset[2]+dataset[4]\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab = {}\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        if num_words is not None:\n",
    "          if i <= num_words:\n",
    "            vocab[word] = i\n",
    "        else:\n",
    "          vocab[word] = i\n",
    "    #vocab = tokenizer.word_index\n",
    "    vocab['<s>'] = len(vocab)+1\n",
    "    vocab['</s>'] = len(vocab)+1\n",
    "    id_vocab = {value: key for key, value in vocab.items()}\n",
    "    return (tokenizer,vocab,id_vocab)\n",
    "\n",
    "tokenizer_obj,vocab,id_vocab = tokenize_c_q_a(train_lists)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size:\",vocab_size)\n",
    "\n",
    "def vectorize_data(tokenizer_obj,train_lists):\n",
    "    context_seq = tokenizer_obj.texts_to_sequences(train_lists[1])\n",
    "    question_seq = tokenizer_obj.texts_to_sequences(train_lists[2])\n",
    "    answer_seq = tokenizer_obj.texts_to_sequences(train_lists[4])\n",
    "    answer_input_seq = [[vocab['<s>']]+i+[vocab['</s>']] for i in answer_seq]\n",
    "    answer_target_seq = [i+[vocab['</s>']] for i in answer_seq]\n",
    "    context_seq_padded = pad_sequences(context_seq,context_maxlen,padding='post', truncating='post')\n",
    "    question_seq_padded = pad_sequences(question_seq,question_maxlen,padding='post', truncating='post')\n",
    "    answer_seq_padded = pad_sequences(answer_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_input_seq_padded = pad_sequences(answer_input_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_target_seq_padded = pad_sequences(answer_target_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_impossible = np.array(train_lists[3])\n",
    "    indices = np.arange(context_seq_padded.shape[0])\n",
    "    np.random.seed(19)\n",
    "    np.random.shuffle(indices)\n",
    "    context_seq_padded = context_seq_padded[indices]\n",
    "    question_seq_padded = question_seq_padded[indices]\n",
    "    answer_seq_padded = answer_seq_padded[indices]\n",
    "    answer_input_seq_padded = answer_input_seq_padded[indices]\n",
    "    answer_target_seq_padded = answer_target_seq_padded[indices]\n",
    "    answer_impossible_shuffled = answer_impossible[indices]\n",
    "    train_samples = int(((context_seq_padded.shape[0]*.8)//128)*128)\n",
    "    end_samples = int((context_seq_padded.shape[0]//128)*128)\n",
    "    train_context_padded_seq = context_seq_padded[:train_samples]\n",
    "    train_question_seq_padded = question_seq_padded[:train_samples]\n",
    "    train_answer_seq_padded = answer_seq_padded[:train_samples]\n",
    "    train_answer_input_seq_padded = answer_input_seq_padded[:train_samples]\n",
    "    train_answer_target_seq_padded = answer_target_seq_padded[:train_samples]\n",
    "    train_answer_impossible = answer_impossible_shuffled[:train_samples]\n",
    "    val_context_padded_seq = context_seq_padded[train_samples:end_samples]\n",
    "    val_question_seq_padded = question_seq_padded[train_samples:end_samples]\n",
    "    val_answer_seq_padded = answer_seq_padded[train_samples:end_samples]\n",
    "    val_answer_input_seq_padded = answer_input_seq_padded[train_samples:end_samples]\n",
    "    val_answer_target_seq_padded = answer_target_seq_padded[train_samples:end_samples]\n",
    "    val_answer_impossible = answer_impossible_shuffled[train_samples:end_samples]\n",
    "    return (train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\n",
    "            train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\n",
    "            val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\n",
    "            val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible)\n",
    "\n",
    "train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\\\n",
    "train_answer_input_seq_padded,train_answer_target_seq_padded,\\\n",
    "train_answer_impossible,\\\n",
    "val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\\\n",
    "val_answer_input_seq_padded,val_answer_target_seq_padded,\\\n",
    "val_answer_impossible\\\n",
    "= vectorize_data(tokenizer_obj,train_lists)\n",
    "\n",
    "print(\"validation num samples where answer impossible: \",len(val_answer_seq_padded[val_answer_impossible==1]))\n",
    "print(\"validation num samples where answer not impossible: \",len(val_answer_seq_padded[val_answer_impossible==0]))\n",
    "print(\"train num samples where answer impossible: \",len(train_answer_seq_padded[train_answer_impossible==1]))\n",
    "print(\"train num samples where answer not impossible: \",len(train_answer_seq_padded[train_answer_impossible==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index,vocab_size=50000,ndim=100):\n",
    "    hands = glove_helper.Hands(ndim)\n",
    "    embedding_matrix = np.zeros((vocab_size+1,ndim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<=vocab_size:\n",
    "            embedding_vector = hands.get_vector(word,strict=False)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "ndim = 100\n",
    "embedding_matrix = create_embedding_matrix(vocab,vocab_size,ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create the Models\n",
    "def create_models(embedding_matrix,\n",
    "                  max_context_length,\n",
    "                  max_question_length,\n",
    "                  max_answer_length,\n",
    "                  num_unit_gru = 64,\n",
    "                  num_layers_gru = 2,\n",
    "                  ndim =100,\n",
    "                  num_episodes = 2,\n",
    "                  num_dense_layer_feasibility_units = 16,\n",
    "                  dropout_rate = 0.5,\n",
    "                  num_dense_layers_feasibility = 1,\n",
    "                  num_episodic_network_unit = 64):\n",
    "    \n",
    "    def create_episodic_memory(num_episodes,\n",
    "                               query,\n",
    "                               context_outputs,\n",
    "                               max_context_length,\n",
    "                               max_question_length,\n",
    "                               num_episodic_network_unit):\n",
    "        m = layers.Lambda(lambda x: x)(query)\n",
    "        weight_layer = layers.Dense(query.shape[1],use_bias=False)\n",
    "        for i in range(num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(query,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(weight_layer(query),1), context_outputs,transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(weight_layer(m),1), context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = layers.concatenate([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = layers.Dense(1)(layers.Dense(num_episodic_network_unit,activation='tanh')(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    #Input Module\n",
    "    context_input = Input(shape=(None,),dtype='int32',name='Context_Input')\n",
    "    context_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                          ndim,\n",
    "                                          mask_zero=True,\n",
    "                                          name='Context_Embedding')(context_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        context_outputs_layers = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                                 dropout=dropout_rate,\n",
    "                                                                 recurrent_dropout= dropout_rate,\n",
    "                                                                 recurrent_initializer='glorot_uniform',\n",
    "                                                                 return_sequences=True),\n",
    "                                                      merge_mode='sum',\n",
    "                                                      name='Context_Bid_Layer'+str(i))\n",
    "        if i==0:\n",
    "            context_outputs = context_outputs_layers(context_embeddings)\n",
    "        else:\n",
    "            context_outputs = context_outputs_layers(context_outputs)\n",
    "        context_outputs = layers.BatchNormalization()(context_outputs)\n",
    "    print(\"Context output shape\",context_outputs.shape)\n",
    "    #Question Module\n",
    "    question_input = Input(shape=(None,),dtype='int32',name='Question_Input')\n",
    "    question_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                           ndim,\n",
    "                                           mask_zero=True,\n",
    "                                           name='Question_Embedding')(question_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        if i==0 and num_layers_gru >1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==0 and num_layers_gru ==1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==(num_layers_gru-1):\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        else:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        question_outputs = layers.BatchNormalization()(question_outputs)\n",
    "    #Episodic Memory Module\n",
    "    m=create_episodic_memory(num_episodes,\n",
    "                             question_outputs,\n",
    "                             context_outputs,\n",
    "                             max_context_length,\n",
    "                             max_question_length,\n",
    "                             num_episodic_network_unit)\n",
    "    #print(m.shape)\n",
    "    #print(context_outputs.shape)\n",
    "    #print(question_outputs.shape)\n",
    "    concatenated_tensor = layers.concatenate(inputs=[m,question_outputs],\n",
    "                                             name='Concatenation_Memory_Question',axis=1)\n",
    "    #answer_module\n",
    "\n",
    "    answer_input = Input(shape=(None,),dtype='int32',name='Answer_Input')\n",
    "    answer_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                         ndim,\n",
    "                                         mask_zero=True,\n",
    "                                         name='Answer_Embedding')(answer_input)\n",
    "    answer_embedding_plus_question = layers.concatenate(\n",
    "                                       inputs = [tf.tile(tf.keras.backend.expand_dims(question_outputs, 1),\n",
    "                                                         tf.constant([1,max_answer_length+2,1],\n",
    "                                                                    tf.int32)),\n",
    "                                                 answer_embeddings],axis=-1)\n",
    "    for i in range(num_layers_gru):\n",
    "        answer_decoder_layers = layers.GRU(num_unit_gru,\n",
    "                                           dropout=dropout_rate,\n",
    "                                           recurrent_dropout= dropout_rate,\n",
    "                                           recurrent_initializer='glorot_uniform',\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           name='Answer_GRU_Layer'+str(i)\n",
    "                                           )\n",
    "        if i==0:\n",
    "            answer_outputs,_ = answer_decoder_layers(answer_embedding_plus_question,initial_state=m)\n",
    "        else:\n",
    "            answer_outputs,_ = answer_decoder_layers(answer_outputs)\n",
    "        answer_outputs = layers.BatchNormalization()(answer_outputs)\n",
    "    answer_decoder_dense = layers.TimeDistributed(layers.Dense(vocab_size+1, activation='softmax')\n",
    "                                                  ,name='Answer_output')\n",
    "    answer_decoder_outputs = answer_decoder_dense(answer_outputs)\n",
    "\n",
    "    answer_model = Model([context_input,question_input,answer_input],answer_decoder_outputs)\n",
    "    answer_model.get_layer(\"Question_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Question_Embedding\").trainable = False\n",
    "    answer_model.get_layer(\"Context_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Context_Embedding\").trainable = False\n",
    "    answer_model.get_layer(\"Answer_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Answer_Embedding\").trainable = False\n",
    "    \n",
    "    #feasibility module\n",
    "    feasibility_input = Input(shape=(concatenated_tensor.shape[1],), name=\"FeasibilityInput\")\n",
    "    for i in range(num_dense_layers_feasibility):\n",
    "        if i==0:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                            activation='relu',name='feasibility_layer_'+str(i))(feasibility_input)\n",
    "        else:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                            activation='relu',name='feasibility_layer_'+str(i))(dense_layer)\n",
    "        dense_layer = layers.BatchNormalization()(dense_layer)\n",
    "        dropout_layer = layers.Dropout(dropout_rate,name='feasibility_drop_'+str(i))(dense_layer)\n",
    "\n",
    "    feasibility_output = layers.Dense(1,activation='sigmoid',name='feasibility_output')(dropout_layer)\n",
    "    feasibility_model = Model(feasibility_input,feasibility_output)\n",
    "\n",
    "    encoder_model = Model([context_input,question_input], [m,concatenated_tensor,question_outputs])\n",
    "    decoder_inputs = answer_input\n",
    "    decoder_state_input_h = Input(shape=(None,), name=\"DecoderStateInput\")\n",
    "    decoder_question_input = Input(shape=(None,),name=\"DecoderQuestionInput\")\n",
    "    decoder_answer_embeddings = answer_model.get_layer('Answer_Embedding')(decoder_inputs)\n",
    "    decoder_answer_embedding_plus_question = layers.concatenate(\n",
    "                                                 inputs = [tf.keras.backend.expand_dims(decoder_question_input, 1),\n",
    "                                                 decoder_answer_embeddings],axis=-1)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        decoder_layers = answer_model.get_layer('Answer_GRU_Layer'+str(i))\n",
    "        if i==0:\n",
    "            decoder_outputs, decoder_state_h = decoder_layers(decoder_answer_embedding_plus_question,\n",
    "                                                              initial_state=decoder_state_input_h)\n",
    "        else:\n",
    "            decoder_outputs, decoder_state_h = decoder_layers(decoder_outputs)\n",
    "\n",
    "    decoder_dense =  answer_model.get_layer('Answer_output')(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model(\n",
    "                        [decoder_inputs] + [decoder_state_input_h]+[decoder_question_input],\n",
    "                        [decoder_dense] + [decoder_state_h])\n",
    "    return (answer_model,encoder_model,decoder_model,feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get sentences from the predicted answers\n",
    "def decode_sequence(context_input_seq,\n",
    "                    question_input_seq,\n",
    "                    encoder_model,\n",
    "                    decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value,_,question_outputs = encoder_model.predict([context_input_seq,question_input_seq])\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    current_step = 0\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab[\"<s>\"]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + [states_value] + [question_outputs])\n",
    "        current_step += 1\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        #print(output_tokens[0,0,0])\n",
    "        #print(output_tokens[0,0,32984])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        #print(sampled_token_index)\n",
    "        if sampled_token_index == 0:\n",
    "            sampled_char = \" \"\n",
    "        else:\n",
    "            sampled_char = id_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char + \" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == '</s>' or len(decoded_sentence) > answer_maxlen:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = h\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Experiment0': {'num_unit_gru': 64,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.001},\n",
       " 'Experiment1': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment2': {'num_unit_gru': 100,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment3': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment4': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 128,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment5': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment_Dic = {'Experiment0': {'num_unit_gru': 64, 'num_layers_gru': 2, 'num_episodes': 3, 'num_dense_layer_feasibility_units': 32, 'dropout_rate': 0.6, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 192, 'learning_rate': 0.001}, 'Experiment1': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 64, 'dropout_rate': 0.7, 'num_dense_layers_feasibility': 3, 'num_episodic_network_unit': 192, 'learning_rate': 0.005}, 'Experiment2': {'num_unit_gru': 100, 'num_layers_gru': 2, 'num_episodes': 3, 'num_dense_layer_feasibility_units': 64, 'dropout_rate': 0.5, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 64, 'learning_rate': 0.005}, 'Experiment3': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 32, 'dropout_rate': 0.7, 'num_dense_layers_feasibility': 3, 'num_episodic_network_unit': 192, 'learning_rate': 0.005}, 'Experiment4': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 48, 'dropout_rate': 0.6, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 128, 'learning_rate': 0.005}, 'Experiment5': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 3, 'num_dense_layer_feasibility_units': 64, 'dropout_rate': 0.5, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 192, 'learning_rate': 0.005}}\n",
    "\n",
    "Experiment_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(Experiment_Dic,\n",
    "                    Experiment_No,\n",
    "                    embedding_matrix,\n",
    "                    ndim = 100,\n",
    "                    tpu_enabled=0,\n",
    "                    num_training_samples=1024,\n",
    "                    num_validation_samples = 256,\n",
    "                    num_epochs = 50,\n",
    "                    batch_size = 10):\n",
    "    num_training_samples = int((num_training_samples//128)*128)\n",
    "    num_validation_samples = int((num_validation_samples//128)*128)\n",
    "    #get the experiment details\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    \n",
    "    \n",
    "    if tpu_enabled==0:\n",
    "        #When GPU ENABLED\n",
    "        answer_model,\\\n",
    "        encoder_model,\\\n",
    "        decoder_model,\\\n",
    "        feasibility_model = create_models(\n",
    "                                      embedding_matrix = embedding_matrix,\n",
    "                                      max_context_length = context_maxlen,\n",
    "                                      max_question_length = question_maxlen,\n",
    "                                      max_answer_length = answer_maxlen,\n",
    "                                      num_unit_gru = num_unit_gru,\n",
    "                                      num_layers_gru = num_layers_gru,\n",
    "                                      ndim =ndim,\n",
    "                                      num_episodes = num_episodes,\n",
    "                                      num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                      dropout_rate = dropout_rate,\n",
    "                                      num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                      num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "        adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        answer_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "                                )\n",
    "\n",
    "        answer_model.summary()\n",
    "        feasibility_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                   )\n",
    "        feasibility_model.summary()\n",
    "    else: \n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "        batch_size = 128*8\n",
    "        with strategy.scope():\n",
    "            answer_model,\\\n",
    "            encoder_model,\\\n",
    "            decoder_model,\\\n",
    "            feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "            adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            answer_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "                                )\n",
    "\n",
    "            answer_model.summary()\n",
    "            feasibility_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                       )\n",
    "            feasibility_model.summary()\n",
    "        \n",
    "        \n",
    "        \n",
    "    history_answer_model = answer_model.fit(\n",
    "                                         {'Context_Input':train_context_padded_seq[:num_training_samples],\n",
    "                                         'Question_Input':train_question_seq_padded[:num_training_samples],\n",
    "                                         'Answer_Input':train_answer_input_seq_padded[:num_training_samples] },\n",
    "                                         {'Answer_output':train_answer_target_seq_padded[:num_training_samples] },\n",
    "                                     epochs=num_epochs,\n",
    "                                     batch_size=batch_size,\n",
    "                                     validation_data=([val_context_padded_seq[:num_validation_samples],\n",
    "                                                       val_question_seq_padded[:num_validation_samples],\n",
    "                                                       val_answer_input_seq_padded[:num_validation_samples]],\n",
    "                                                       val_answer_target_seq_padded[:num_validation_samples]))\n",
    "    answer_model.save(ExperimentNo+'original_model_answer_model.h5')\n",
    "    encoder_model.save(ExperimentNo+'original_model_encoder_model.h5')\n",
    "    decoder_model.save(ExperimentNo+'original_model_decoder_model.h5')\n",
    "    with open(ExperimentNo+'original_model_'+'history_answer_model', 'wb') as file_history:\n",
    "        pickle.dump(history_answer_model.history, file_history)\n",
    "    _,encoder_prediction,_ = encoder_model.predict([train_context_padded_seq[:num_training_samples],\n",
    "                                                    train_question_seq_padded[:num_training_samples]])\n",
    "    _,encoder_validation_prediction,_ = encoder_model.predict([val_context_padded_seq[:num_validation_samples],\n",
    "                                                               val_question_seq_padded[:num_validation_samples]])\n",
    "    history_feasibility_model = feasibility_model.fit(encoder_prediction,\n",
    "                                                      train_answer_impossible[:num_training_samples],\n",
    "                                                      epochs=num_epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      validation_data = \n",
    "                                                            (encoder_validation_prediction,\n",
    "                                                             val_answer_impossible[:num_validation_samples])\n",
    "                                                      )\n",
    "\n",
    "\n",
    "\n",
    "    feasibility_model.save(ExperimentNo+'original_model_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'original_model_'+'history_feasibility_model', 'wb') as file_history:\n",
    "        pickle.dump(history_feasibility_model.history, file_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_model(Experiment_Dic,\n",
    "                           Experiment_No,\n",
    "                           embedding_matrix,\n",
    "                           ndim = 100):\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    \n",
    "    inference_answer_model,\\\n",
    "    inference_encoder_model,\\\n",
    "    inference_decoder_model,\\\n",
    "    inference_feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "\n",
    "    inference_answer_model.load_weights(ExperimentNo+'original_model_answer_model.h5')\n",
    "    inference_encoder_model.load_weights(ExperimentNo+'original_model_encoder_model.h5')\n",
    "    inference_decoder_model.load_weights(ExperimentNo+'original_model_decoder_model.h5')\n",
    "    inference_feasibility_model.load_weights(ExperimentNo+'original_model_feasibility_model.h5')\n",
    "    return (inference_answer_model,inference_encoder_model,inference_decoder_model,inference_feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context output shape (None, None, 64)\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Question_Input (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Question_Embedding (Embedding)  (None, None, 100)    8870200     Question_Input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Question_Bid_Layer0 (Bidirectio (None, None, 64)     63744       Question_Embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, 64)     256         Question_Bid_Layer0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Context_Input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Question_Bid_Layer1 (Bidirectio (None, 64)           49920       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Context_Embedding (Embedding)   (None, None, 100)    8870200     Context_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64)           256         Question_Bid_Layer1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Context_Bid_Layer0 (Bidirection (None, None, 64)     63744       Context_Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, 64)     256         Context_Bid_Layer0[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 64)           0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Context_Bid_Layer1 (Bidirection (None, None, 64)     49920       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_42 (Tens [(None, 1, 64)]      0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_43 (Tens [(None, 1, 64)]      0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 64)           4096        batch_normalization_24[0][0]     \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "                                                                 tf_op_layer_Sum_9[0][0]          \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "                                                                 tf_op_layer_Sum_10[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, 64)     256         Context_Bid_Layer1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_21 (TensorFlow [(None, 212, 64)]    0           tf_op_layer_ExpandDims_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_22 (TensorFlow [(None, 212, 64)]    0           tf_op_layer_ExpandDims_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_44 (Tens [(None, 1, 64)]      0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_45 (Tens [(None, 1, 64)]      0           dense_24[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_18 (Subtract)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_19 (Subtract)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_18 (TensorFl [(None, 1, None)]    0           tf_op_layer_ExpandDims_44[0][0]  \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_19 (TensorFl [(None, 1, None)]    0           tf_op_layer_ExpandDims_45[0][0]  \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_21[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_18 (TensorFlowO [(None, 212, 64)]    0           subtract_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_19 (TensorFlowO [(None, 212, 64)]    0           subtract_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_18 (Permute)            (None, None, 1)      0           tf_op_layer_MatMul_18[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "permute_19 (Permute)            (None, None, 1)      0           tf_op_layer_MatMul_19[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 212, 450)     0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_21[0][0]        \n",
      "                                                                 tf_op_layer_Tile_22[0][0]        \n",
      "                                                                 multiply_9[0][0]                 \n",
      "                                                                 multiply_10[0][0]                \n",
      "                                                                 tf_op_layer_Abs_18[0][0]         \n",
      "                                                                 tf_op_layer_Abs_19[0][0]         \n",
      "                                                                 permute_18[0][0]                 \n",
      "                                                                 permute_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 212, 192)     86592       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 212, 1)       193         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_18 (Tenso [(None, 1, 212)]     0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_9 (TensorFl [(None, 1, 212)]     0           tf_op_layer_transpose_18[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_19 (Tenso [(None, 212, 1)]     0           tf_op_layer_Softmax_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_9 (TensorFlowOp [(None, 212, 64)]    0           tf_op_layer_transpose_19[0][0]   \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_9 (TensorFlowOp [(None, 64)]         0           tf_op_layer_mul_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_46 (Tens [(None, 1, 64)]      0           tf_op_layer_Sum_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_47 (Tens [(None, 1, 64)]      0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_23 (TensorFlow [(None, 212, 64)]    0           tf_op_layer_ExpandDims_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_24 (TensorFlow [(None, 212, 64)]    0           tf_op_layer_ExpandDims_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_48 (Tens [(None, 1, 64)]      0           dense_24[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_49 (Tens [(None, 1, 64)]      0           dense_24[3][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_20 (Subtract)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_21 (Subtract)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_20 (TensorFl [(None, 1, None)]    0           tf_op_layer_ExpandDims_48[0][0]  \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_21 (TensorFl [(None, 1, None)]    0           tf_op_layer_ExpandDims_49[0][0]  \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_23[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_20 (TensorFlowO [(None, 212, 64)]    0           subtract_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_21 (TensorFlowO [(None, 212, 64)]    0           subtract_21[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_20 (Permute)            (None, None, 1)      0           tf_op_layer_MatMul_20[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "permute_21 (Permute)            (None, None, 1)      0           tf_op_layer_MatMul_21[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 212, 450)     0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_23[0][0]        \n",
      "                                                                 tf_op_layer_Tile_24[0][0]        \n",
      "                                                                 multiply_11[0][0]                \n",
      "                                                                 multiply_12[0][0]                \n",
      "                                                                 tf_op_layer_Abs_20[0][0]         \n",
      "                                                                 tf_op_layer_Abs_21[0][0]         \n",
      "                                                                 permute_20[0][0]                 \n",
      "                                                                 permute_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 212, 192)     86592       concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 212, 1)       193         dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_20 (Tenso [(None, 1, 212)]     0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_10 (TensorF [(None, 1, 212)]     0           tf_op_layer_transpose_20[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_21 (Tenso [(None, 212, 1)]     0           tf_op_layer_Softmax_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_10 (TensorFlowO [(None, 212, 64)]    0           tf_op_layer_transpose_21[0][0]   \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_10 (TensorFlowO [(None, 64)]         0           tf_op_layer_mul_10[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_50 (Tens [(None, 1, 64)]      0           tf_op_layer_Sum_10[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_51 (Tens [(None, 1, 64)]      0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_25 (TensorFlow [(None, 212, 64)]    0           tf_op_layer_ExpandDims_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_26 (TensorFlow [(None, 212, 64)]    0           tf_op_layer_ExpandDims_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_52 (Tens [(None, 1, 64)]      0           dense_24[4][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_53 (Tens [(None, 1, 64)]      0           dense_24[5][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "subtract_22 (Subtract)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_23 (Subtract)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_22 (TensorFl [(None, 1, None)]    0           tf_op_layer_ExpandDims_52[0][0]  \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_23 (TensorFl [(None, 1, None)]    0           tf_op_layer_ExpandDims_53[0][0]  \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 212, 64)      0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_22 (TensorFlowO [(None, 212, 64)]    0           subtract_22[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_23 (TensorFlowO [(None, 212, 64)]    0           subtract_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "permute_22 (Permute)            (None, None, 1)      0           tf_op_layer_MatMul_22[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "permute_23 (Permute)            (None, None, 1)      0           tf_op_layer_MatMul_23[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 212, 450)     0           batch_normalization_22[0][0]     \n",
      "                                                                 tf_op_layer_Tile_25[0][0]        \n",
      "                                                                 tf_op_layer_Tile_26[0][0]        \n",
      "                                                                 multiply_13[0][0]                \n",
      "                                                                 multiply_14[0][0]                \n",
      "                                                                 tf_op_layer_Abs_22[0][0]         \n",
      "                                                                 tf_op_layer_Abs_23[0][0]         \n",
      "                                                                 permute_22[0][0]                 \n",
      "                                                                 permute_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 212, 192)     86592       concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 212, 1)       193         dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_22 (Tenso [(None, 1, 212)]     0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_11 (TensorF [(None, 1, 212)]     0           tf_op_layer_transpose_22[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_54 (Tens [(None, 1, 64)]      0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Answer_Input (InputLayer)       [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_23 (Tenso [(None, 212, 1)]     0           tf_op_layer_Softmax_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_27 (TensorFlow [(None, 12, 64)]     0           tf_op_layer_ExpandDims_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Answer_Embedding (Embedding)    (None, None, 100)    8870200     Answer_Input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_11 (TensorFlowO [(None, 212, 64)]    0           tf_op_layer_transpose_23[0][0]   \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 12, 164)      0           tf_op_layer_Tile_27[0][0]        \n",
      "                                                                 Answer_Embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_11 (TensorFlowO [(None, 64)]         0           tf_op_layer_mul_11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Answer_GRU_Layer0 (GRU)         multiple             44160       concatenate_18[0][0]             \n",
      "                                                                 tf_op_layer_Sum_11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 12, 64)       256         Answer_GRU_Layer0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Answer_GRU_Layer1 (GRU)         multiple             24960       batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 12, 64)       256         Answer_GRU_Layer1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Answer_output (TimeDistributed) multiple             5765630     batch_normalization_26[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 32,938,665\n",
      "Trainable params: 6,327,297\n",
      "Non-trainable params: 26,611,368\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FeasibilityInput (InputLayer [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "feasibility_layer_0 (Dense)  (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "feasibility_drop_0 (Dropout) (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "feasibility_output (Dense)   (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,289\n",
      "Trainable params: 4,225\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Train on 1024 samples, validate on 256 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 156s 152ms/sample - loss: 3.7945 - sparse_categorical_accuracy: 0.1045 - val_loss: 3.5886 - val_sparse_categorical_accuracy: 0.1304\n",
      "Epoch 2/50\n",
      "1024/1024 [==============================] - 135s 132ms/sample - loss: 2.9195 - sparse_categorical_accuracy: 0.2477 - val_loss: 3.2220 - val_sparse_categorical_accuracy: 0.2014\n",
      "Epoch 3/50\n",
      "1024/1024 [==============================] - 134s 130ms/sample - loss: 2.0790 - sparse_categorical_accuracy: 0.3681 - val_loss: 2.2529 - val_sparse_categorical_accuracy: 0.4115\n",
      "Epoch 4/50\n",
      "1024/1024 [==============================] - 134s 131ms/sample - loss: 1.5904 - sparse_categorical_accuracy: 0.4274 - val_loss: 2.0181 - val_sparse_categorical_accuracy: 0.4348\n",
      "Epoch 5/50\n",
      "1024/1024 [==============================] - 125s 122ms/sample - loss: 1.4860 - sparse_categorical_accuracy: 0.4354 - val_loss: 2.0688 - val_sparse_categorical_accuracy: 0.4446\n",
      "Epoch 6/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.4509 - sparse_categorical_accuracy: 0.4385 - val_loss: 2.0607 - val_sparse_categorical_accuracy: 0.4494\n",
      "Epoch 7/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.4290 - sparse_categorical_accuracy: 0.4411 - val_loss: 2.0609 - val_sparse_categorical_accuracy: 0.4514\n",
      "Epoch 8/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.4005 - sparse_categorical_accuracy: 0.4455 - val_loss: 2.0582 - val_sparse_categorical_accuracy: 0.4523\n",
      "Epoch 9/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.3904 - sparse_categorical_accuracy: 0.4440 - val_loss: 2.0605 - val_sparse_categorical_accuracy: 0.4523\n",
      "Epoch 10/50\n",
      "1024/1024 [==============================] - 126s 123ms/sample - loss: 1.3745 - sparse_categorical_accuracy: 0.4428 - val_loss: 2.0521 - val_sparse_categorical_accuracy: 0.4543\n",
      "Epoch 11/50\n",
      "1024/1024 [==============================] - 126s 123ms/sample - loss: 1.3635 - sparse_categorical_accuracy: 0.4457 - val_loss: 2.0560 - val_sparse_categorical_accuracy: 0.4553\n",
      "Epoch 12/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.3496 - sparse_categorical_accuracy: 0.4464 - val_loss: 2.0565 - val_sparse_categorical_accuracy: 0.4553\n",
      "Epoch 13/50\n",
      "1024/1024 [==============================] - 123s 121ms/sample - loss: 1.3419 - sparse_categorical_accuracy: 0.4452 - val_loss: 2.0520 - val_sparse_categorical_accuracy: 0.4562\n",
      "Epoch 14/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.3343 - sparse_categorical_accuracy: 0.4510 - val_loss: 2.0531 - val_sparse_categorical_accuracy: 0.4553\n",
      "Epoch 15/50\n",
      "1024/1024 [==============================] - 121s 118ms/sample - loss: 1.3225 - sparse_categorical_accuracy: 0.4467 - val_loss: 2.0448 - val_sparse_categorical_accuracy: 0.4562\n",
      "Epoch 16/50\n",
      "1024/1024 [==============================] - 121s 118ms/sample - loss: 1.3146 - sparse_categorical_accuracy: 0.4512 - val_loss: 2.0483 - val_sparse_categorical_accuracy: 0.4562\n",
      "Epoch 17/50\n",
      "1024/1024 [==============================] - 120s 117ms/sample - loss: 1.3006 - sparse_categorical_accuracy: 0.4519 - val_loss: 2.0486 - val_sparse_categorical_accuracy: 0.4582\n",
      "Epoch 18/50\n",
      "1024/1024 [==============================] - 121s 118ms/sample - loss: 1.2924 - sparse_categorical_accuracy: 0.4510 - val_loss: 2.0472 - val_sparse_categorical_accuracy: 0.4562\n",
      "Epoch 19/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.2820 - sparse_categorical_accuracy: 0.4539 - val_loss: 2.0423 - val_sparse_categorical_accuracy: 0.4611\n",
      "Epoch 20/50\n",
      "1024/1024 [==============================] - 120s 118ms/sample - loss: 1.2747 - sparse_categorical_accuracy: 0.4536 - val_loss: 2.0448 - val_sparse_categorical_accuracy: 0.4630\n",
      "Epoch 21/50\n",
      "1024/1024 [==============================] - 120s 117ms/sample - loss: 1.2658 - sparse_categorical_accuracy: 0.4563 - val_loss: 2.0450 - val_sparse_categorical_accuracy: 0.4660\n",
      "Epoch 22/50\n",
      "1024/1024 [==============================] - 120s 118ms/sample - loss: 1.2518 - sparse_categorical_accuracy: 0.4572 - val_loss: 2.0434 - val_sparse_categorical_accuracy: 0.4669\n",
      "Epoch 23/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.2492 - sparse_categorical_accuracy: 0.4592 - val_loss: 2.0426 - val_sparse_categorical_accuracy: 0.4679\n",
      "Epoch 24/50\n",
      "1024/1024 [==============================] - 131s 128ms/sample - loss: 1.2405 - sparse_categorical_accuracy: 0.4565 - val_loss: 2.0484 - val_sparse_categorical_accuracy: 0.4669\n",
      "Epoch 25/50\n",
      "1024/1024 [==============================] - 127s 124ms/sample - loss: 1.2355 - sparse_categorical_accuracy: 0.4606 - val_loss: 2.0510 - val_sparse_categorical_accuracy: 0.4698\n",
      "Epoch 26/50\n",
      "1024/1024 [==============================] - 126s 123ms/sample - loss: 1.2233 - sparse_categorical_accuracy: 0.4611 - val_loss: 2.0484 - val_sparse_categorical_accuracy: 0.4718\n",
      "Epoch 27/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.2180 - sparse_categorical_accuracy: 0.4623 - val_loss: 2.0474 - val_sparse_categorical_accuracy: 0.4728\n",
      "Epoch 28/50\n",
      "1024/1024 [==============================] - 128s 125ms/sample - loss: 1.2049 - sparse_categorical_accuracy: 0.4623 - val_loss: 2.0457 - val_sparse_categorical_accuracy: 0.4718\n",
      "Epoch 29/50\n",
      "1024/1024 [==============================] - 126s 123ms/sample - loss: 1.2015 - sparse_categorical_accuracy: 0.4608 - val_loss: 2.0495 - val_sparse_categorical_accuracy: 0.4698\n",
      "Epoch 30/50\n",
      "1024/1024 [==============================] - 125s 122ms/sample - loss: 1.1850 - sparse_categorical_accuracy: 0.4659 - val_loss: 2.0451 - val_sparse_categorical_accuracy: 0.4689\n",
      "Epoch 31/50\n",
      "1024/1024 [==============================] - 125s 122ms/sample - loss: 1.1840 - sparse_categorical_accuracy: 0.4616 - val_loss: 2.0511 - val_sparse_categorical_accuracy: 0.4728\n",
      "Epoch 32/50\n",
      "1024/1024 [==============================] - 127s 124ms/sample - loss: 1.1751 - sparse_categorical_accuracy: 0.4644 - val_loss: 2.0514 - val_sparse_categorical_accuracy: 0.4737\n",
      "Epoch 33/50\n",
      "1024/1024 [==============================] - 125s 122ms/sample - loss: 1.1692 - sparse_categorical_accuracy: 0.4635 - val_loss: 2.0552 - val_sparse_categorical_accuracy: 0.4767\n",
      "Epoch 34/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.1615 - sparse_categorical_accuracy: 0.4656 - val_loss: 2.0559 - val_sparse_categorical_accuracy: 0.4776\n",
      "Epoch 35/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.1582 - sparse_categorical_accuracy: 0.4649 - val_loss: 2.0549 - val_sparse_categorical_accuracy: 0.4776\n",
      "Epoch 36/50\n",
      "1024/1024 [==============================] - 125s 122ms/sample - loss: 1.1446 - sparse_categorical_accuracy: 0.4652 - val_loss: 2.0583 - val_sparse_categorical_accuracy: 0.4805\n",
      "Epoch 37/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.1349 - sparse_categorical_accuracy: 0.4668 - val_loss: 2.0609 - val_sparse_categorical_accuracy: 0.4815\n",
      "Epoch 38/50\n",
      "1024/1024 [==============================] - 126s 123ms/sample - loss: 1.1328 - sparse_categorical_accuracy: 0.4697 - val_loss: 2.0746 - val_sparse_categorical_accuracy: 0.4815\n",
      "Epoch 39/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.1208 - sparse_categorical_accuracy: 0.4741 - val_loss: 2.0696 - val_sparse_categorical_accuracy: 0.4786\n",
      "Epoch 40/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.1145 - sparse_categorical_accuracy: 0.4724 - val_loss: 2.0674 - val_sparse_categorical_accuracy: 0.4805\n",
      "Epoch 41/50\n",
      "1024/1024 [==============================] - 124s 122ms/sample - loss: 1.1076 - sparse_categorical_accuracy: 0.4721 - val_loss: 2.0732 - val_sparse_categorical_accuracy: 0.4815\n",
      "Epoch 42/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.0964 - sparse_categorical_accuracy: 0.4714 - val_loss: 2.0706 - val_sparse_categorical_accuracy: 0.4805\n",
      "Epoch 43/50\n",
      "1024/1024 [==============================] - 124s 121ms/sample - loss: 1.0903 - sparse_categorical_accuracy: 0.4716 - val_loss: 2.0750 - val_sparse_categorical_accuracy: 0.4825\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 137s 134ms/sample - loss: 1.0882 - sparse_categorical_accuracy: 0.4741 - val_loss: 2.0757 - val_sparse_categorical_accuracy: 0.4815\n",
      "Epoch 45/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.0777 - sparse_categorical_accuracy: 0.4726 - val_loss: 2.0803 - val_sparse_categorical_accuracy: 0.4815\n",
      "Epoch 46/50\n",
      "1024/1024 [==============================] - 127s 124ms/sample - loss: 1.0748 - sparse_categorical_accuracy: 0.4753 - val_loss: 2.0838 - val_sparse_categorical_accuracy: 0.4776\n",
      "Epoch 47/50\n",
      "1024/1024 [==============================] - 125s 122ms/sample - loss: 1.0655 - sparse_categorical_accuracy: 0.4743 - val_loss: 2.0841 - val_sparse_categorical_accuracy: 0.4796\n",
      "Epoch 48/50\n",
      "1024/1024 [==============================] - 128s 125ms/sample - loss: 1.0522 - sparse_categorical_accuracy: 0.4769 - val_loss: 2.0877 - val_sparse_categorical_accuracy: 0.4786\n",
      "Epoch 49/50\n",
      "1024/1024 [==============================] - 123s 120ms/sample - loss: 1.0485 - sparse_categorical_accuracy: 0.4784 - val_loss: 2.0879 - val_sparse_categorical_accuracy: 0.4786\n",
      "Epoch 50/50\n",
      "1024/1024 [==============================] - 129s 126ms/sample - loss: 1.0374 - sparse_categorical_accuracy: 0.4774 - val_loss: 2.0903 - val_sparse_categorical_accuracy: 0.4796\n",
      "Train on 1024 samples, validate on 256 samples\n",
      "Epoch 1/50\n",
      "1024/1024 [==============================] - 1s 917us/sample - loss: 0.7612 - binary_accuracy: 0.5850 - val_loss: 0.8289 - val_binary_accuracy: 0.6016\n",
      "Epoch 2/50\n",
      "1024/1024 [==============================] - 0s 202us/sample - loss: 0.6650 - binary_accuracy: 0.6553 - val_loss: 0.7054 - val_binary_accuracy: 0.6016\n",
      "Epoch 3/50\n",
      "1024/1024 [==============================] - 0s 193us/sample - loss: 0.6361 - binary_accuracy: 0.6592 - val_loss: 0.7033 - val_binary_accuracy: 0.6016\n",
      "Epoch 4/50\n",
      "1024/1024 [==============================] - 0s 254us/sample - loss: 0.6392 - binary_accuracy: 0.6670 - val_loss: 0.6903 - val_binary_accuracy: 0.6016\n",
      "Epoch 5/50\n",
      "1024/1024 [==============================] - 0s 237us/sample - loss: 0.6273 - binary_accuracy: 0.6641 - val_loss: 0.6801 - val_binary_accuracy: 0.6016\n",
      "Epoch 6/50\n",
      "1024/1024 [==============================] - 0s 213us/sample - loss: 0.6209 - binary_accuracy: 0.6777 - val_loss: 0.6935 - val_binary_accuracy: 0.6016\n",
      "Epoch 7/50\n",
      "1024/1024 [==============================] - 0s 191us/sample - loss: 0.6038 - binary_accuracy: 0.6855 - val_loss: 0.6949 - val_binary_accuracy: 0.5977\n",
      "Epoch 8/50\n",
      "1024/1024 [==============================] - 0s 194us/sample - loss: 0.6097 - binary_accuracy: 0.6719 - val_loss: 0.6902 - val_binary_accuracy: 0.5820\n",
      "Epoch 9/50\n",
      "1024/1024 [==============================] - 0s 201us/sample - loss: 0.6205 - binary_accuracy: 0.6631 - val_loss: 0.6818 - val_binary_accuracy: 0.6016\n",
      "Epoch 10/50\n",
      "1024/1024 [==============================] - 0s 202us/sample - loss: 0.6155 - binary_accuracy: 0.6768 - val_loss: 0.6870 - val_binary_accuracy: 0.5977\n",
      "Epoch 11/50\n",
      "1024/1024 [==============================] - 0s 187us/sample - loss: 0.6058 - binary_accuracy: 0.6836 - val_loss: 0.6864 - val_binary_accuracy: 0.5859\n",
      "Epoch 12/50\n",
      "1024/1024 [==============================] - 0s 179us/sample - loss: 0.5961 - binary_accuracy: 0.6787 - val_loss: 0.6855 - val_binary_accuracy: 0.5742\n",
      "Epoch 13/50\n",
      "1024/1024 [==============================] - 0s 182us/sample - loss: 0.5957 - binary_accuracy: 0.6904 - val_loss: 0.6891 - val_binary_accuracy: 0.5898\n",
      "Epoch 14/50\n",
      "1024/1024 [==============================] - 0s 189us/sample - loss: 0.5957 - binary_accuracy: 0.6973 - val_loss: 0.7010 - val_binary_accuracy: 0.5938\n",
      "Epoch 15/50\n",
      "1024/1024 [==============================] - 0s 190us/sample - loss: 0.5925 - binary_accuracy: 0.6768 - val_loss: 0.7004 - val_binary_accuracy: 0.5820\n",
      "Epoch 16/50\n",
      "1024/1024 [==============================] - 0s 182us/sample - loss: 0.5956 - binary_accuracy: 0.6855 - val_loss: 0.7014 - val_binary_accuracy: 0.5586\n",
      "Epoch 17/50\n",
      "1024/1024 [==============================] - 0s 186us/sample - loss: 0.6019 - binary_accuracy: 0.6797 - val_loss: 0.6927 - val_binary_accuracy: 0.5820\n",
      "Epoch 18/50\n",
      "1024/1024 [==============================] - 0s 173us/sample - loss: 0.5969 - binary_accuracy: 0.6865 - val_loss: 0.6969 - val_binary_accuracy: 0.5781\n",
      "Epoch 19/50\n",
      "1024/1024 [==============================] - 0s 168us/sample - loss: 0.5975 - binary_accuracy: 0.6855 - val_loss: 0.6872 - val_binary_accuracy: 0.5898\n",
      "Epoch 20/50\n",
      "1024/1024 [==============================] - 0s 168us/sample - loss: 0.5931 - binary_accuracy: 0.6846 - val_loss: 0.6875 - val_binary_accuracy: 0.6094\n",
      "Epoch 21/50\n",
      "1024/1024 [==============================] - 0s 168us/sample - loss: 0.6005 - binary_accuracy: 0.6709 - val_loss: 0.6853 - val_binary_accuracy: 0.5938\n",
      "Epoch 22/50\n",
      "1024/1024 [==============================] - 0s 166us/sample - loss: 0.5926 - binary_accuracy: 0.6748 - val_loss: 0.7044 - val_binary_accuracy: 0.5781\n",
      "Epoch 23/50\n",
      "1024/1024 [==============================] - 0s 168us/sample - loss: 0.5891 - binary_accuracy: 0.6807 - val_loss: 0.7064 - val_binary_accuracy: 0.6016\n",
      "Epoch 24/50\n",
      "1024/1024 [==============================] - 0s 170us/sample - loss: 0.5980 - binary_accuracy: 0.6836 - val_loss: 0.7223 - val_binary_accuracy: 0.5820\n",
      "Epoch 25/50\n",
      "1024/1024 [==============================] - 0s 166us/sample - loss: 0.5992 - binary_accuracy: 0.6846 - val_loss: 0.6978 - val_binary_accuracy: 0.6016\n",
      "Epoch 26/50\n",
      "1024/1024 [==============================] - 0s 150us/sample - loss: 0.5995 - binary_accuracy: 0.6826 - val_loss: 0.6963 - val_binary_accuracy: 0.5859\n",
      "Epoch 27/50\n",
      "1024/1024 [==============================] - 0s 153us/sample - loss: 0.5974 - binary_accuracy: 0.6787 - val_loss: 0.7161 - val_binary_accuracy: 0.5781\n",
      "Epoch 28/50\n",
      "1024/1024 [==============================] - 0s 154us/sample - loss: 0.5890 - binary_accuracy: 0.6895 - val_loss: 0.7073 - val_binary_accuracy: 0.5938\n",
      "Epoch 29/50\n",
      "1024/1024 [==============================] - 0s 158us/sample - loss: 0.5899 - binary_accuracy: 0.6895 - val_loss: 0.7164 - val_binary_accuracy: 0.5820\n",
      "Epoch 30/50\n",
      "1024/1024 [==============================] - 0s 160us/sample - loss: 0.5888 - binary_accuracy: 0.6934 - val_loss: 0.7111 - val_binary_accuracy: 0.5977\n",
      "Epoch 31/50\n",
      "1024/1024 [==============================] - 0s 163us/sample - loss: 0.5981 - binary_accuracy: 0.6992 - val_loss: 0.7257 - val_binary_accuracy: 0.5742\n",
      "Epoch 32/50\n",
      "1024/1024 [==============================] - 0s 163us/sample - loss: 0.5985 - binary_accuracy: 0.6768 - val_loss: 0.7054 - val_binary_accuracy: 0.5859\n",
      "Epoch 33/50\n",
      "1024/1024 [==============================] - 0s 163us/sample - loss: 0.5921 - binary_accuracy: 0.6738 - val_loss: 0.7247 - val_binary_accuracy: 0.5820\n",
      "Epoch 34/50\n",
      "1024/1024 [==============================] - 0s 164us/sample - loss: 0.5950 - binary_accuracy: 0.6904 - val_loss: 0.7035 - val_binary_accuracy: 0.6055\n",
      "Epoch 35/50\n",
      "1024/1024 [==============================] - 0s 161us/sample - loss: 0.5849 - binary_accuracy: 0.6914 - val_loss: 0.7201 - val_binary_accuracy: 0.5820\n",
      "Epoch 36/50\n",
      "1024/1024 [==============================] - 0s 163us/sample - loss: 0.5836 - binary_accuracy: 0.6934 - val_loss: 0.6998 - val_binary_accuracy: 0.5938\n",
      "Epoch 37/50\n",
      "1024/1024 [==============================] - 0s 164us/sample - loss: 0.5860 - binary_accuracy: 0.6885 - val_loss: 0.7042 - val_binary_accuracy: 0.5977\n",
      "Epoch 38/50\n",
      "1024/1024 [==============================] - 0s 162us/sample - loss: 0.5905 - binary_accuracy: 0.6768 - val_loss: 0.7173 - val_binary_accuracy: 0.5977\n",
      "Epoch 39/50\n",
      "1024/1024 [==============================] - 0s 160us/sample - loss: 0.5874 - binary_accuracy: 0.6953 - val_loss: 0.7223 - val_binary_accuracy: 0.5859\n",
      "Epoch 40/50\n",
      "1024/1024 [==============================] - 0s 164us/sample - loss: 0.5967 - binary_accuracy: 0.6943 - val_loss: 0.7078 - val_binary_accuracy: 0.5859\n",
      "Epoch 41/50\n",
      "1024/1024 [==============================] - 0s 162us/sample - loss: 0.5846 - binary_accuracy: 0.7002 - val_loss: 0.7112 - val_binary_accuracy: 0.6055\n",
      "Epoch 42/50\n",
      "1024/1024 [==============================] - 0s 162us/sample - loss: 0.5821 - binary_accuracy: 0.6943 - val_loss: 0.7222 - val_binary_accuracy: 0.5859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50\n",
      "1024/1024 [==============================] - 0s 164us/sample - loss: 0.5819 - binary_accuracy: 0.6904 - val_loss: 0.7350 - val_binary_accuracy: 0.5820\n",
      "Epoch 44/50\n",
      "1024/1024 [==============================] - 0s 162us/sample - loss: 0.5883 - binary_accuracy: 0.6914 - val_loss: 0.7296 - val_binary_accuracy: 0.6016\n",
      "Epoch 45/50\n",
      "1024/1024 [==============================] - 0s 162us/sample - loss: 0.5872 - binary_accuracy: 0.6973 - val_loss: 0.7454 - val_binary_accuracy: 0.5898\n",
      "Epoch 46/50\n",
      "1024/1024 [==============================] - 0s 161us/sample - loss: 0.5861 - binary_accuracy: 0.6885 - val_loss: 0.7129 - val_binary_accuracy: 0.6055\n",
      "Epoch 47/50\n",
      "1024/1024 [==============================] - 0s 162us/sample - loss: 0.5857 - binary_accuracy: 0.6865 - val_loss: 0.7153 - val_binary_accuracy: 0.5938\n",
      "Epoch 48/50\n",
      "1024/1024 [==============================] - 0s 162us/sample - loss: 0.5831 - binary_accuracy: 0.7021 - val_loss: 0.7271 - val_binary_accuracy: 0.5977\n",
      "Epoch 49/50\n",
      "1024/1024 [==============================] - 0s 164us/sample - loss: 0.5929 - binary_accuracy: 0.6924 - val_loss: 0.7179 - val_binary_accuracy: 0.6055\n",
      "Epoch 50/50\n",
      "1024/1024 [==============================] - 0s 170us/sample - loss: 0.5805 - binary_accuracy: 0.6875 - val_loss: 0.7243 - val_binary_accuracy: 0.6016\n"
     ]
    }
   ],
   "source": [
    "#Run Experiments\n",
    "#change the num_training_samples and num_validation_samples make sure its multiple of 128 when running on tpu\n",
    "#change to tpu_enabled = 1 when running on tpu \n",
    "#Change the batch_size and epochs too\n",
    "run_experiments(Experiment_Dic=Experiment_Dic,\n",
    "                Experiment_No=0,\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                ndim = 100,\n",
    "                tpu_enabled=0,\n",
    "                num_training_samples=1024,\n",
    "                num_validation_samples = 256,\n",
    "                num_epochs = 50,\n",
    "                batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Inference\n",
    "inference_answer_model,\\\n",
    "inference_encoder_model,\\\n",
    "inference_decoder_model,\\\n",
    "inference_feasibility_model = create_inference_model(Experiment_Dic=Experiment_Dic,\n",
    "                                                     Experiment_No=0,\n",
    "                                                     embedding_matrix=embedding_matrix,\n",
    "                                                     ndim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what train line connects florida to the north\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> amtrak operating numerous lines throughout connecting the state s largest cities\n",
      "question: who jointly owns sportsnet and tsn\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: who revealed the starting points of this evolution to be economic deterioration and political confusion\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> ervin staub </s>\n",
      "question: what is yeovilton home to\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> the royal naval air station in yeovilton is one of britain\n",
      "question: what are two smaller galleries in nanjing\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> red chamber art garden and jinling stone gallery </s>\n",
      "question: where did shuhda work\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: in what year did apple face multiple intellectual property lawsuits\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> 2005 </s>\n",
      "question: how many people lived in kaliningrad in 1946\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: who was the artist that painted the fresco that replaced the mosaic at santa sobina\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> taddeo zuccari </s>\n",
      "question: what was different about the bass line in the hip hop influenced chicago songs\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(3000,3010):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(context_input_seq,\n",
    "                                       question_input_seq,\n",
    "                                       inference_encoder_model,\n",
    "                                       inference_decoder_model)\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in train_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print('Predicted Answer:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu_2",
   "language": "python",
   "name": "tensorflow_cpu_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
